{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1550bcb",
   "metadata": {},
   "source": [
    "# Importation des librairies nécessaires à notre environnement\n",
    "* on reprend les imports [des 6 modules Python](https://github.com/javaskater/rcp217_project/tree/main/PYTHON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530e18e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From DataLoader modules\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import FloatTensor, IntTensor\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "# From RESNET1D and averaging modukes\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "# From project 16 module\n",
    "import torch.nn.functional as TF\n",
    "# From the training module\n",
    "import datetime  # for timestamping our training evolution\n",
    "import torch.optim as optim\n",
    "# from the validation module\n",
    "## standard Python\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d5681b",
   "metadata": {},
   "source": [
    "# partie DataSource creation\n",
    "* reprend le code du module [Création de DataSource avec partie main pour test en environnement virtualisé local](https://github.com/javaskater/rcp217_project/blob/main/PYTHON/dataloader_creation.py)\n",
    "* L'explication correspondante est en [lien Markdown](https://github.com/javaskater/rcp217_project/blob/main/docs/PYTHON_STEPS/1-DATALOADER.md)\n",
    "## TODO \n",
    "* reprendre dans [le TP10](https://jhub3.cnam.fr/user/24592/lab/tree/MonDossier/RCP217/TP10_07042025/TP02_corrige.ipynb)\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "# creating the training and validation set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2, stratify = y)\n",
    "```\n",
    "* résoudre le problème des labels en output de la DataSource cf [Question 3 posée au professeur](https://github.com/javaskater/rcp217_project/blob/main/docs/Questions/3-PYTORCH_TENSORS.md)\n",
    "* Trouver le chemin Drive Google Colab pour mes répertoires de Séries Temporelles générées par R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6647f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDatasetForPCoefficients(Dataset):\n",
    "    def __init__(self, list_of_pathes_to_the_timeseries_files: list):\n",
    "        super().__init__()\n",
    "        self.X = [] # les Time series\n",
    "        self.y = [] # le coefficient p correspondant\n",
    "        for path_to_the_timeseries_files in list_of_pathes_to_the_timeseries_files:\n",
    "            for fichier_time_serie_name in os.listdir(path_to_the_timeseries_files):\n",
    "                if fichier_time_serie_name.endswith(\".csv\"):\n",
    "                    coeff_pq_search = re.search('ar_([0-9]{1})__ma_([0-9]{1})', fichier_time_serie_name, re.IGNORECASE)\n",
    "                    if coeff_pq_search:\n",
    "                        p_str = coeff_pq_search.group(1)\n",
    "                        p = int(p_str)\n",
    "                        q_str = coeff_pq_search.group(2)\n",
    "                        q = int(q_str)\n",
    "                        chemin_complet_fichier_time_serie = os.path.join(path_to_the_timeseries_files, fichier_time_serie_name)\n",
    "                        print(f\"[TimeSeriesDatasetForPCoefficients: init] fichier trouvé {chemin_complet_fichier_time_serie} avec pattern pour p oefficient: {p} et pour q coefficient: {q}\")\n",
    "                        with open(chemin_complet_fichier_time_serie, mode ='r')as file:\n",
    "                            csvFile = csv.reader(file, delimiter =';')\n",
    "                            next(csvFile) # skip the header\n",
    "                            for ligne_time_serie_str in csvFile:\n",
    "                                ligne_time_serie = FloatTensor(list(map(float, ligne_time_serie_str)))\n",
    "                                print(ligne_time_serie)\n",
    "                                self.X.append(ligne_time_serie)\n",
    "                        p_tensor = IntTensor([p]) # Ce doit être un tenseur avec une seule donnée: l'ordre attenduexit()\n",
    "                        self.y.append(p_tensor)\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.X) == len(self.y)\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        assert index < len(self.y)\n",
    "        return self.X[index], self.y[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47c1e7d",
   "metadata": {},
   "source": [
    "# Partie 2 et 3 du réseau de neuronnes\n",
    "## ce module est commun \n",
    "* Etape 2 du réseau de Neuronnes: 4 convolutions RESNET 1D avec kernel de 10 et Stride de 1\n",
    "* Etape 3 du réseau de Neuronnes: 6 convolutions RESNET 1D avec kernel de 1 et Stride de 1\n",
    "## liens\n",
    "* [le module lui même avec une partie main pour test](https://github.com/javaskater/rcp217_project/blob/main/PYTHON/resnet1d_module.py)\n",
    "* [la documentation Markdown sur ce module](https://github.com/javaskater/rcp217_project/blob/main/docs/PYTHON_STEPS/2-RESNET.md) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ab9411",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet1D(nn.Module):\n",
    "\n",
    "    def __init__(self,in_channels,out_channels,kernel_size,stride, debug=False):\n",
    "\n",
    "        super(ResNet1D,self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.debug = debug\n",
    "    \n",
    "\n",
    "        # commonly used relu\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # 1x1\n",
    "        self.conv1_1x1 = nn.Conv1d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=kernel_size, stride=stride, padding=0, bias=False )\n",
    "        self.batchnorm1 = nn.BatchNorm1d(self.out_channels)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # input stored to be added before the final relu\n",
    "        in_x = x\n",
    "\n",
    "        # conv1x1->BN->relu\n",
    "        x = self.conv1_1x1(x)\n",
    "        if self.debug:\n",
    "            print(f\"[ResNet1D/forward] after CONV shape of x {x.shape}\")\n",
    "        \n",
    "        x = self.batchnorm1(x)\n",
    "        if self.debug:\n",
    "            print(f\"[ResNet1D/forward] after BatcNorm shape of x {x.shape}\")\n",
    "        \n",
    "        x = self.relu(x)\n",
    "        if self.debug:\n",
    "            print(f\"[ResNet1D/forward] after RELU shape of x {x.shape}\")\n",
    "\n",
    "\n",
    "        # identity with trucation\n",
    "        in_x_truncated = in_x[...,:x.shape[2]]\n",
    "        x_out = x + in_x_truncated\n",
    "\n",
    "        # final relu\n",
    "        x_out_after_relu = self.relu(x_out)\n",
    "        \n",
    "        return x_out_after_relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5639665e",
   "metadata": {},
   "source": [
    "# Dernière étape du réseau de Neurones\n",
    "* Convolution passage de 300 à 10 feature maps\n",
    "* et moyenne des données sur chaque couche (feature map) \n",
    "* après cette étape ce sera un softmax\n",
    "## lien\n",
    "* plus d'explications sur cette partie [lien vers le Markdown d'explication](https://github.com/javaskater/rcp217_project/blob/main/docs/PYTHON_STEPS/4-AVERAGING.md)\n",
    "* [le code du module standalone avec une partie main de test](https://github.com/javaskater/rcp217_project/blob/main/PYTHON/averaging_module.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4a7afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AVERAGING(nn.Module):\n",
    "\n",
    "    def __init__(self, debug=False):\n",
    "\n",
    "        super(AVERAGING,self).__init__()\n",
    "    \n",
    "        self.debug = debug\n",
    "\n",
    "        # commonly used relu\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # 1x1\n",
    "        self.conv1_before_averging = nn.Conv1d(in_channels=300, out_channels=10, kernel_size=10, stride=1, padding=0, bias=False )\n",
    "        self.batchnorm_before_averging= nn.BatchNorm1d(10)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        # conv1x1->BN->relu\n",
    "        x = self.conv1_before_averging(x)\n",
    "        x = self.batchnorm_before_averging(x)\n",
    "        x = self.relu(x)\n",
    "        if self.debug:\n",
    "            print(f\"[AVERAGING/forward] after CONV/NORM/RELU shape of x {x.shape}\")\n",
    "        \n",
    "        x = torch.mean(x, 2) # calculate the mean on the last dimension (batch size, number of )\n",
    "        if self.debug:\n",
    "            print(f\"[AVERAGING/forward] after mean operation shape of x {x.shape}\")\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c7c54b",
   "metadata": {},
   "source": [
    "# Le réseau dans son ensemble\n",
    "* c'est le réseau complet sous forme de nn.Module Pytorch (comme les 2 modules précédents), nom de la classe représentant le réseau **Project16**\n",
    "* Fait appel au 2 modules Pytorch précédents\n",
    "# liens\n",
    "* plus d'explications sur cette partie [lien vers le Markdown d'explication](https://github.com/javaskater/rcp217_project/blob/main/docs/PYTHON_STEPS/5-GLOBAL.md)\n",
    "* [le code du module standalone avec une partie main de test](https://github.com/javaskater/rcp217_project/blob/main/PYTHON/projetct16_module.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02df332",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Project16(nn.Module):\n",
    "\n",
    "    def __init__(self, debug=False):\n",
    "\n",
    "        super(Project16,self).__init__()\n",
    "\n",
    "        self.debug = debug\n",
    "\n",
    "        # commonly used relu\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # from 1 time serie to 300 series of length 100 each \n",
    "        self.conv1_init = nn.Conv1d(in_channels=1, out_channels=300, kernel_size=10, stride=10, padding=0, bias=False)\n",
    "\n",
    "        # 4 RESNET 1D with kernel of 10\n",
    "        self.resnet10_1 = ResNet1D(300, 300, 10, 1, debug=debug)\n",
    "        self.resnet10_2 = ResNet1D(300, 300, 10, 1, debug=debug)\n",
    "        self.resnet10_3 = ResNet1D(300, 300, 10, 1, debug=debug)\n",
    "        self.resnet10_4 = ResNet1D(300, 300, 10, 1, debug=debug)\n",
    "\n",
    "        # 6 RESNET 1D with kernel of 1\n",
    "        self.resnet1_1 = ResNet1D(300, 300, 1, 1, debug=debug)\n",
    "        self.resnet1_2 = ResNet1D(300, 300, 1, 1, debug=debug)\n",
    "        self.resnet1_3 = ResNet1D(300, 300, 1, 1, debug=debug)\n",
    "        self.resnet1_4 = ResNet1D(300, 300, 1, 1, debug=debug)\n",
    "        self.resnet1_5 = ResNet1D(300, 300, 1, 1, debug=debug)\n",
    "        self.resnet1_6 = ResNet1D(300, 300, 1, 1, debug=debug)\n",
    "\n",
    "        # The last averaging module\n",
    "        self.averaging = AVERAGING(debug=self.debug)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        # conv1x1->BN->relu\n",
    "        x = self.conv1_init(x)\n",
    "        # first relu\n",
    "        x = self.relu(x)\n",
    "        if self.debug:\n",
    "            print(f\"[Project16/forward] after first convolution and relu shape of x {x.shape}\")\n",
    "        \n",
    "        # 4 RESNET 1D with kernel of 10\n",
    "        x = self.resnet10_1(x)\n",
    "        x = self.resnet10_2(x)\n",
    "        x = self.resnet10_3(x)\n",
    "        x = self.resnet10_4(x)\n",
    "        if self.debug:\n",
    "            print(f\"[Project16/forward] after 4 RESNET1D Blocks of kernel 10 shape of x {x.shape}\")\n",
    "        \n",
    "        # 6 RESNET 1D with kernel of 1\n",
    "        x = self.resnet1_1(x)\n",
    "        x = self.resnet1_2(x)\n",
    "        x = self.resnet1_3(x)\n",
    "        x = self.resnet1_4(x)\n",
    "        x = self.resnet1_5(x)\n",
    "        x = self.resnet1_6(x)\n",
    "        if self.debug:\n",
    "            print(f\"[Project16/forward] after 6 RESNET1D Blocks of kernel 1 shape of x {x.shape}\")\n",
    "\n",
    "        # from 300 to 10 feature maps and Averaging each map and outputting 10 softmax values\n",
    "        x = self.averaging(x)\n",
    "        x = TF.softmax(x,dim=1)\n",
    "        if self.debug:\n",
    "            print(f\"[Project16/forward] after Averaging and softmax Blocks shape of x {x.shape}\")\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efed852c",
   "metadata": {},
   "source": [
    "# Lancement de l'entrainement\n",
    "* le code reprend [ce code Python à lancer dans notre environnement virtualisé](https://github.com/javaskater/rcp217_project/blob/main/PYTHON/training_module.py) \n",
    "## TODO\n",
    "* résoudre le problème de label à savoir comment le mettre en *Pytorch Tensor* cf. [la réponse 4 de ce Post Stackoverflow](https://stackoverflow.com/questions/63825841/attributeerror-tuple-has-no-attribute-to)\n",
    "  * soit comme entier simple (de 0 à 9)\n",
    "  * soit comme Tensor one hot vector\n",
    "## lien vers la documentation\n",
    "* C'est [le même lien que précédemment vers la documentation avec les sorties en erreur à corriger]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3c0004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the Manning book's source code https://github.com/deep-learning-with-pytorch/dlwpt-code/blob/master/p1ch8/1_convolution.ipynb\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader, device, debug=False):\n",
    "    for epoch in range(1, n_epochs + 1):  # <2>\n",
    "        loss_train = 0.0\n",
    "        for series, labels in train_loader:  # <3>\n",
    "            if debug == True:\n",
    "                print(f\"[training_loop] series batch before unsqueeze {series.shape}\")                \n",
    "            series = series.unsqueeze(1) # add the one channel dimension betwwen the batch_size dimension and the serie's length\n",
    "            if debug == True:\n",
    "                print(f\"[training_loop] series batch after unsqueeze {series.shape}\") \n",
    "            series = series.to(device)\n",
    "            labels = labels.squeeze(1).long() # cf erreur 3 et 4 dans le compte rendu\n",
    "            labels = labels.to(device=device)\n",
    "              \n",
    "            \n",
    "            outputs = model(series)  # <4>\n",
    "            \n",
    "            if debug == True:\n",
    "                print(f\"[training_loop] outputs shape {outputs.shape} labels shape {labels.shape}\")\n",
    "\n",
    "            loss = loss_fn(outputs,labels)  # <5>\n",
    "\n",
    "            optimizer.zero_grad()  # <6>\n",
    "            \n",
    "            loss.backward()  # <7>\n",
    "            \n",
    "            optimizer.step()  # <8>\n",
    "\n",
    "            loss_train += loss.item()  # <9>\n",
    "\n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))  # <10>\n",
    "\n",
    "data_train = TimeSeriesDatasetForPCoefficients(['/home/jpmena/CONSULTANT/CNAM/rcp217_project/R/ts_generees_28082025'])\n",
    "train_loader = DataLoader(data_train, batch_size=50, shuffle=True)\n",
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")\n",
    "model = Project16().to(device=device)  # We use the Datanetwork just created \n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)  #  <3>\n",
    "loss_fn = nn.CrossEntropyLoss()  #  <4>\n",
    "\n",
    "training_loop(  # <5>\n",
    "    n_epochs = 50,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    device = device,\n",
    "    debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1e556e",
   "metadata": {},
   "source": [
    "# La validation\n",
    "* Dans cette partie, on teste le pourcentage de données exactes pour le paramètre p\n",
    "  * et sur les données qui on servi pour l'entrainement (train)\n",
    "  * et sur les données mises de côté pour la validation (test) \n",
    "## TODO\n",
    "* pas encore testé\n",
    "* prendre des données différentes de la validation (faire un nouveau lancement de la génération des Time Series)\n",
    "## liens\n",
    "* [le code en standalone (environnement local virtualisé)](https://github.com/javaskater/rcp217_project/blob/main/PYTHON/validation_module.py)\n",
    "  * noter que l'on recharge les coefficients du modèle sauvegardés suite au [lancement du training en environnement local virtualisé en Stand Alone](https://github.com/javaskater/rcp217_project/blob/main/PYTHON/training_module.py) \n",
    "* [Les explications et remarques sont en fin de ce MArkdown](https://github.com/javaskater/rcp217_project/blob/main/docs/PYTHON_STEPS/5-GLOBAL.md#validation-todo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde23cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the Manning book's source code https://github.com/deep-learning-with-pytorch/dlwpt-code/blob/master/p1ch8/1_convolution.ipynb\n",
    "def validate(model, train_loader, val_loader):\n",
    "    accdict = {}\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for series, labels in loader:\n",
    "                series = series.to(device=device)\n",
    "                labels = torch.tensor(labels).squeeze(1).long().to(device=device) # cf erreur 3 et 4 dans le compte rendu\n",
    "                series2D = series.unsqueeze(1) # add the one channel dimension between the batch_size dimension and the serie's length\n",
    "                outputs = model(series2D) \n",
    "                _, predicted = torch.max(outputs, dim=1) # <1>\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == labels).sum())\n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "        accdict[name] = correct / total\n",
    "    return accdict\n",
    "\n",
    "data_path = '../R/' # the coeeficients are at the same level as my time series\n",
    "data_train = TimeSeriesDatasetForPCoefficients([data_path + 'ts_generees_28082025'])\n",
    "train_loader = DataLoader(data_train, batch_size=50, shuffle=True)\n",
    "# TODO prendre une série générée différente\n",
    "data_val = TimeSeriesDatasetForPCoefficients([data_path + 'ts_generees_28082025'])\n",
    "val_loader = DataLoader(data_val, batch_size=50, shuffle=True)\n",
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")\n",
    "\n",
    "loaded_model = model\n",
    "\n",
    "optimizer = optim.SGD(loaded_model.parameters(), lr=1e-2)  #  <3>\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()  #  <4>\n",
    "\n",
    "# we launch the validation on the training set and the validation set only to compare\n",
    "all_acc_dict = collections.OrderedDict()\n",
    "all_acc_dict[\"baseline\"] = validate(loaded_model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
